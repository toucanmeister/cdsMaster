\relax 
\abx@aux@refcontext{none/global//global/global}
\abx@aux@cite{0}{gibbs}
\abx@aux@segm{0}{0}{gibbs}
\abx@aux@cite{0}{metropolishastings}
\abx@aux@segm{0}{0}{metropolishastings}
\abx@aux@cite{0}{hmc}
\abx@aux@segm{0}{0}{hmc}
\abx@aux@cite{0}{slice_sampling}
\abx@aux@segm{0}{0}{slice_sampling}
\abx@aux@cite{0}{annealed_importance_sampling}
\abx@aux@segm{0}{0}{annealed_importance_sampling}
\abx@aux@cite{0}{bayesian_cubature}
\abx@aux@segm{0}{0}{bayesian_cubature}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{hmc_in_ns}
\abx@aux@segm{0}{0}{hmc_in_ns}
\@writefile{toc}{\contentsline {chapter}{Introduction}{5}{}\protected@file@percent }
\abx@aux@cite{0}{jaynes}
\abx@aux@segm{0}{0}{jaynes}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Background}{6}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Bayesian Statistics}{6}{}\protected@file@percent }
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\newlabel{eq:bayes}{{1.1}{7}}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\newlabel{eq:evidence}{{1.2}{8}}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Fitting Bayesian Models}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Analytical and Numerical Methods}{9}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Prior $pi(\theta ) = P(\theta )$, likelihood $L(\theta ) = P(H T H H H \,|\, \theta )$ and posterior $P(\theta \,|\, H T H H H)$ of $\theta $ in the coin example with $\pi (\theta ) = 2 \cdot (1-x)$ and data: H T H H H\relax }}{10}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:coin_example}{{1.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Monte Carlo Methods}{10}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Markov Chain Monte Carlo Methods}{11}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{betancourt_talk}
\abx@aux@segm{0}{0}{betancourt_talk}
\abx@aux@cite{0}{bayesian_cubature}
\abx@aux@segm{0}{0}{bayesian_cubature}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{liebloss}
\abx@aux@segm{0}{0}{liebloss}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Nested Sampling}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}From a multi- to a one-dimensional Integral}{13}{}\protected@file@percent }
\abx@aux@cite{0}{measure_theory}
\abx@aux@segm{0}{0}{measure_theory}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the relationship between a likelihood threshold $\lambda $ and the corresponding prior mass $\mu (\lambda )$ for a uniform prior (red) and a likelihood (blue).\relax }}{14}{}\protected@file@percent }
\newlabel{fig:xlambda}{{1.2}{14}}
\newlabel{eq:tonelli}{{1.3}{14}}
\newlabel{eq:integral_2}{{1.4}{14}}
\newlabel{eq:mu_definition}{{1.5}{14}}
\newlabel{eq:integral_3}{{1.6}{15}}
\newlabel{eq:integral_4}{{1.7}{15}}
\newlabel{eq:integral_5}{{1.8}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Sampling and Sorting}{15}{}\protected@file@percent }
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{measure_theory}
\abx@aux@segm{0}{0}{measure_theory}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of how the evidence integral can be approximated using prior mass samples $X_i$ sorted by their likelihoods, in this case using an upper sum. The shaded region is the approximate evidence. Based on a graphic in \blx@tocontentsinit {0}\cite [4]{skilling}.\relax }}{16}{}\protected@file@percent }
\newlabel{fig:integral_approximate}{{1.3}{16}}
\newlabel{eq:sample_ordering}{{1.9}{16}}
\newlabel{eq:approx_evidence}{{1.10}{16}}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\@writefile{toc}{\contentsline {subsubsection}{Proof: Prior Masses of Prior Samples are Uniformly Distributed}{17}{}\protected@file@percent }
\newlabel{eq:L_integral}{{1.11}{17}}
\newlabel{eq:dirac_delta_property}{{1.12}{17}}
\abx@aux@cite{0}{measure_theory}
\abx@aux@segm{0}{0}{measure_theory}
\abx@aux@cite{0}{casellaberger}
\abx@aux@segm{0}{0}{casellaberger}
\abx@aux@cite{0}{computational_stats}
\abx@aux@segm{0}{0}{computational_stats}
\newlabel{eq:tonelli2}{{1.13}{18}}
\newlabel{eq:delta_property}{{1.14}{18}}
\newlabel{eq:L_cdf}{{1.15}{18}}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{beta_dist}
\abx@aux@segm{0}{0}{beta_dist}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Introducing a Likelihood-Constraint}{19}{}\protected@file@percent }
\abx@aux@cite{0}{nested_sampling_talk}
\abx@aux@segm{0}{0}{nested_sampling_talk}
\abx@aux@cite{0}{kld}
\abx@aux@segm{0}{0}{kld}
\abx@aux@cite{0}{nested_sampling_talk}
\abx@aux@segm{0}{0}{nested_sampling_talk}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The basic Nested Sampling algorithm.}}{21}{}\protected@file@percent }
\newlabel{alg:nested_sampling}{{1}{21}}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Intermediate states of two Nested Sampling runs on an eggcrate-shaped likelihood with a uniform prior. Black crosses are dead points, red crosses are live points. All live points are inside the current likelihood-constraint contour. The likelihood is visualized by the colored background, lighter colors represent higher likelihood values. The left image shows an earlier iteration than the right one, as can be seen by the lower number of dead points and the looser likelihood-constraint contours.\relax }}{22}{}\protected@file@percent }
\newlabel{fig:ns_eggcrate_example}{{1.4}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Likelihood-restricted prior sampling}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Rejection Sampling}{22}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{automala}
\abx@aux@segm{0}{0}{automala}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Discretized LRPS}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Metropolis Algorithm}{23}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Likelihood-restricted prior sampling using Metropolis.}}{23}{}\protected@file@percent }
\newlabel{alg:lrpsmetropolis}{{2}{23}}
\abx@aux@cite{0}{hmc}
\abx@aux@segm{0}{0}{hmc}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Visualization of the Metropolis algorithm in a two-dimensional sample space. The black line indicates the likelihood-constraint-boundary, the dot is the starting point and each cross is a sampled point.\relax }}{24}{}\protected@file@percent }
\newlabel{fig:metropolis_example}{{1.5}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Hamiltonian Monte Carlo}{24}{}\protected@file@percent }
\abx@aux@cite{0}{hmc_in_ns}
\abx@aux@segm{0}{0}{hmc_in_ns}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Visualization of HMC in a two-dimensional sample space. Contour lines and shading indicate the negative log probability of the target distribution (a mixture of two Gaussians). The dot is the starting point and each cross is a sampled point.\relax }}{25}{}\protected@file@percent }
\newlabel{fig:hmc_example}{{1.6}{25}}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Left: An iteration of HMC for LRPS with reflection and a chain of five samples on a standard-normal-likelihood-restricted uniform prior space. The black dot is the starting spot, each blue cross is a generated sample. Right: A later iteration of the same example. Note how every integration step contains a reflection and how far away the reflection points are from the actual boundary.\relax }}{26}{}\protected@file@percent }
\newlabel{fig:hmc_reflection_example}{{1.7}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}The Barrier Method}{26}{}\protected@file@percent }
\newlabel{eq:cvxproblem}{{1.16}{26}}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\newlabel{eq:cvxproblem_unconstrained}{{1.17}{27}}
\newlabel{eq:exact_barrier_function}{{1.18}{27}}
\newlabel{eq:log_barrier_function}{{1.19}{27}}
\newlabel{eq:barrierproblem}{{1.20}{27}}
\newlabel{eq:objective}{{1.21}{27}}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Visualization of the exact barrier function $I$ in blue, and three log barrier functions (with $t=1,4,16$) in red. The log barrier becomes a better approximation as $t$ increases.\relax }}{28}{}\protected@file@percent }
\newlabel{fig:logbarrier}{{1.8}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Central path of an example optimization problem. The shaded area is the feasible region, given by the constraint functions in black. The grey lines indicate contours of the objective function, with the unconstrained optimum at the grey dot. The central path of the problem is shown in red. $x^*(1)$ is marked by a red cross.\relax }}{29}{}\protected@file@percent }
\newlabel{fig:barrier_method_example}{{1.9}{29}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces The basic Barrier Method.}}{29}{}\protected@file@percent }
\newlabel{alg:barrier_method}{{3}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Nested Sampling with Barriers}{30}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Derivation}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Deriving the log barrier term}{31}{}\protected@file@percent }
\newlabel{eq:last_optimization_problem}{{2.1}{32}}
\newlabel{eq:log_barrier_term}{{2.2}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Incorporating the log barrier term}{32}{}\protected@file@percent }
\abx@aux@cite{0}{collapsed_gibbs}
\abx@aux@segm{0}{0}{collapsed_gibbs}
\newlabel{eq:recognize_bayes}{{2.3}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualization of the unnormalized CDF $\Pi _q$ of $q$ for different values of $t$ and $q_{\textrm  {max}}=5$.\relax }}{35}{}\protected@file@percent }
\newlabel{fig:unnormalized}{{2.1}{35}}
\abx@aux@cite{0}{bayesian_data_analysis}
\abx@aux@segm{0}{0}{bayesian_data_analysis}
\newlabel{eq:sub_zq_1}{{2.4}{36}}
\newlabel{eq:gamma_diff}{{2.5}{36}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Nested Sampling with the log barrier term.}}{37}{}\protected@file@percent }
\newlabel{alg:barriersampling}{{4}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Interpretation}{37}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of the normalized PDF (left) and normalized CDF (right) of $q$ for different values of $t$ and $q_{\textrm  {max}}=2$.\relax }}{38}{}\protected@file@percent }
\newlabel{fig:q_normalized}{{2.2}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Interpreting $q$}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Influence of $t$ and $q_{\textrm  {max}}$}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Implementation of LRPS}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Rejection Sampling and Discretized LRPS}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Metropolis Algorithm}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Hamiltonian Monte Carlo}{39}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A run of HMC for LRPS with a chain of five samples on a standard-normal-likelihood-restricted uniform prior space. The black dot is the starting spot, each black cross is a generated sample. The background color shows the negative log density the particles move on, with lighter colors representing higher values.\relax }}{40}{}\protected@file@percent }
\newlabel{fig:barrier_sampling_hmc}{{2.3}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Results}{40}{}\protected@file@percent }
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{sphere_sampling}
\abx@aux@segm{0}{0}{sphere_sampling}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Nested Sampling with and without Barriers}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Estimated Evidence}{42}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Posterior Modes Found}{42}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Box plot of each method's evidence estimate's deviation from the true value. Top: $L_1$-example, Bottom: $L_2$-example. Orange lines are the median values, gray crosses are the mean values.\relax }}{43}{}\protected@file@percent }
\newlabel{fig:results_logZ_diffs}{{2.4}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Box plots of how many of the two modes were found by each method. The values represent $\frac  {m}{2}$, where $m$ is the number of modes found. Top: $L_1$-example, Bottom: $L_2$-example. Orange lines are the median values, gray crosses are the mean values.\relax }}{43}{}\protected@file@percent }
\newlabel{fig:results_modes_found}{{2.5}{43}}
\@writefile{toc}{\contentsline {subsubsection}{Number of Iterations}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Acceptance Rate}{44}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Box plots of the KLD of estimated and true mode weights. Top: $L_1$-example, Bottom: $L_2$-example. The results for HMC with reflection is presented on a separate scale to allow better comparison between the others. Orange lines are the median values, gray crosses are the mean values.\relax }}{45}{}\protected@file@percent }
\newlabel{fig:results_KLDs}{{2.6}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Box plots of how many iterations each method took to reach the stopping point. Top: $L_1$-example, Bottom: $L_2$-example. Orange lines are the median values, gray crosses are the mean values.\relax }}{45}{}\protected@file@percent }
\newlabel{fig:results_iterations}{{2.7}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Box plots of the ratio of accepted proposals to total proposals made during LRPS for each method. Top: $L_1$-example, Bottom: $L_2$-example. Orange lines are the median values, gray crosses are the mean values.\relax }}{46}{}\protected@file@percent }
\newlabel{fig:results_acceptance_rates}{{2.8}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Influence of $t$ and $q_{\textrm  {max}}$}{46}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Box plot of each configuration's evidence estimate's deviation from the true value. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{47}{}\protected@file@percent }
\newlabel{fig:results_params_logZ}{{2.9}{47}}
\@writefile{toc}{\contentsline {subsubsection}{Estimated Evidence}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Posterior Modes Found}{47}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Box plots of how many of the two modes were found by each method. The values represent $\frac  {m}{2}$, where $m$ is the number of modes found. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{48}{}\protected@file@percent }
\newlabel{fig:results_params_modes_found}{{2.10}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Box plots of the KLD of estimated and true mode weights. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{48}{}\protected@file@percent }
\newlabel{fig:results_params_KLDs}{{2.11}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Box plots of how many iterations each configuration took to reach the stopping point. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{49}{}\protected@file@percent }
\newlabel{fig:results_params_iterations}{{2.12}{49}}
\@writefile{toc}{\contentsline {subsubsection}{Number of Iterations}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Acceptance Rate}{49}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Box plots of the ratio of accepted proposals to total proposals made during LRPS for each configuration. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{50}{}\protected@file@percent }
\newlabel{fig:results_params_acceptance_rates}{{2.13}{50}}
\abx@aux@cite{0}{ultranest}
\abx@aux@segm{0}{0}{ultranest}
\abx@aux@cite{0}{dynesty}
\abx@aux@segm{0}{0}{dynesty}
\abx@aux@cite{0}{jaxns}
\abx@aux@segm{0}{0}{jaxns}
\abx@aux@cite{0}{polychord}
\abx@aux@segm{0}{0}{polychord}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Conclusions}{51}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{54}{}\protected@file@percent }
\abx@aux@cite{0}{casellaberger}
\abx@aux@segm{0}{0}{casellaberger}
\@writefile{toc}{\contentsline {chapter}{Appendix}{55}{}\protected@file@percent }
\newlabel{eq:prior_mass_integral_appendix}{{3.1}{55}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Visualization of the integral in (3.1\hbox {}). The shaded region is the integral, equivalent to $\mu (L^*) - \mu (\lambda )$.\relax }}{56}{}\protected@file@percent }
\newlabel{fig:vis_prior_masses_lrps_appendix}{{3.1}{56}}
\abx@aux@read@bbl@mdfivesum{D684AC38BED003D0A69E908B782D0AAB}
\abx@aux@defaultrefcontext{0}{gibbs}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{metropolishastings}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hmc}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{slice_sampling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{annealed_importance_sampling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesian_cubature}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{skilling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hmc_in_ns}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{jaynes}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mcelreath}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mckay}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{betancourt_talk}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{liebloss}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{measure_theory}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{casellaberger}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{computational_stats}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{beta_dist}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nested_sampling_talk}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kld}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{automala}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{boyd}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{collapsed_gibbs}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesian_data_analysis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{sphere_sampling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ultranest}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{dynesty}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{jaxns}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{polychord}{none/global//global/global}
\gdef \@abspage@last{60}
