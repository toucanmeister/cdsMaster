\relax 
\abx@aux@refcontext{none/global//global/global}
\abx@aux@cite{0}{gibbs}
\abx@aux@segm{0}{0}{gibbs}
\abx@aux@cite{0}{metropolishastings}
\abx@aux@segm{0}{0}{metropolishastings}
\abx@aux@cite{0}{hmc}
\abx@aux@segm{0}{0}{hmc}
\abx@aux@cite{0}{slice_sampling}
\abx@aux@segm{0}{0}{slice_sampling}
\abx@aux@cite{0}{annealed_importance_sampling}
\abx@aux@segm{0}{0}{annealed_importance_sampling}
\abx@aux@cite{0}{bayesian_cubature}
\abx@aux@segm{0}{0}{bayesian_cubature}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{hmc_in_ns}
\abx@aux@segm{0}{0}{hmc_in_ns}
\@writefile{toc}{\contentsline {chapter}{Introduction}{5}{}\protected@file@percent }
\abx@aux@cite{0}{jaynes}
\abx@aux@segm{0}{0}{jaynes}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Background}{6}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Bayesian Statistics}{6}{}\protected@file@percent }
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\newlabel{eq:bayes}{{1.1}{7}}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\newlabel{eq:evidence}{{1.2}{8}}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Prior, likelihood and posterior of $\theta $ in the coin example with $\pi (\theta ) = 2 \cdot (1-x)$ and data: H T H H H\relax }}{9}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:coin_example}{{1.1}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Fitting Bayesian Models}{9}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{betancourt_talk}
\abx@aux@segm{0}{0}{betancourt_talk}
\abx@aux@cite{0}{bayesian_cubature}
\abx@aux@segm{0}{0}{bayesian_cubature}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{liebloss}
\abx@aux@segm{0}{0}{liebloss}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Nested Sampling}{12}{}\protected@file@percent }
\abx@aux@cite{0}{measure_theory}
\abx@aux@segm{0}{0}{measure_theory}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}From a multi- to a one-dimensional Integral}{13}{}\protected@file@percent }
\newlabel{eq:tonelli}{{1.3}{13}}
\newlabel{eq:integral_2}{{1.4}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the relationship between a likelihood threshold $\lambda $ and the corresponding prior mass $\mu (\lambda )$ for a uniform prior (purple) and a likelihood (blue).\relax }}{14}{}\protected@file@percent }
\newlabel{fig:xlambda}{{1.2}{14}}
\newlabel{eq:integral_3}{{1.5}{14}}
\newlabel{eq:integral_4}{{1.6}{14}}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\newlabel{eq:integral_5}{{1.7}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Sampling and Sorting}{15}{}\protected@file@percent }
\newlabel{eq:sample_ordering}{{1.8}{15}}
\newlabel{eq:approx_evidence}{{1.9}{15}}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of how the evidence integral can be approximated using prior mass samples $X_i$ sorted by their likelihoods. The shaded region is the approximate evidence. Based on a graphic in \blx@tocontentsinit {0}\cite [4]{skilling}.\relax }}{16}{}\protected@file@percent }
\newlabel{fig:integral_approximate}{{1.3}{16}}
\@writefile{toc}{\contentsline {subsubsection}{Proof: Prior Masses of Prior Samples are Uniformly Distributed}{16}{}\protected@file@percent }
\newlabel{eq:L_integral}{{1.10}{16}}
\newlabel{eq:dirac_delta_property}{{1.11}{16}}
\abx@aux@cite{0}{casellaberger}
\abx@aux@segm{0}{0}{casellaberger}
\abx@aux@cite{0}{computational_stats}
\abx@aux@segm{0}{0}{computational_stats}
\newlabel{eq:L_cdf}{{1.14}{17}}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Introducing a Likelihood-Constraint}{18}{}\protected@file@percent }
\abx@aux@cite{0}{nested_sampling_talk}
\abx@aux@segm{0}{0}{nested_sampling_talk}
\abx@aux@cite{0}{nested_sampling_talk}
\abx@aux@segm{0}{0}{nested_sampling_talk}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The basic Nested Sampling algorithm.}}{20}{}\protected@file@percent }
\newlabel{alg:nested_sampling}{{1}{20}}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Intermediate states of two Nested Sampling runs on an eggcrate-shaped likelihood with a uniform prior. Black crosses are dead points, red crosses are live points. All live points are inside the current likelihood-constraint contour. The likelihood is visualized by the colored background, lighter colors represent higher likelihood values. The left image shows an earlier iteration than the right one, as can be seen by the lower number of dead points and the looser likelihood-constraint contours.\relax }}{21}{}\protected@file@percent }
\newlabel{fig:ns_eggcrate_example}{{1.4}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Likelihood-restricted prior sampling}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Rejection Sampling}{21}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{automala}
\abx@aux@segm{0}{0}{automala}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Discretized LRPS}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Metropolis Algorithm}{22}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Likelihood-restricted prior sampling using Metropolis.}}{22}{}\protected@file@percent }
\newlabel{alg:lrpsmetropolis}{{2}{22}}
\abx@aux@cite{0}{hmc}
\abx@aux@segm{0}{0}{hmc}
\abx@aux@cite{0}{hmc_in_ns}
\abx@aux@segm{0}{0}{hmc_in_ns}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Hamiltonian Monte Carlo}{23}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Visualization of HMC in a two-dimensional sample space. Contour lines and shading indicate the negative log probability of the target distribution (a mixture of two Gaussians). The dot is the starting point and each cross is a sampled point.\relax }}{24}{}\protected@file@percent }
\newlabel{fig:hmc_example}{{1.5}{24}}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Left: An iteration of HMC for LRPS with reflection and a chain of five samples on a standard-normal-likelihood-restricted uniform prior space. The black dot is the starting spot, each blue cross is a generated sample. Right: An later iteration on the same example. Note how every integration step contains a reflection and how far away the reflection points are from the actual boundary.\relax }}{25}{}\protected@file@percent }
\newlabel{fig:hmc_reflection_example}{{1.6}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}The Barrier Method}{25}{}\protected@file@percent }
\newlabel{eq:cvxproblem}{{1.15}{25}}
\newlabel{eq:cvxproblem_unconstrained}{{1.16}{25}}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Visualization of the exact barrier function $I$ in blue, and three log barrier functions (with $t=1,4,16$) in purple. The log barrier becomes a better approximation as $t$ increases.\relax }}{26}{}\protected@file@percent }
\newlabel{fig:logbarrier}{{1.7}{26}}
\newlabel{eq:barrierproblem}{{1.17}{26}}
\newlabel{eq:objective}{{1.18}{26}}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Central path of an example optimization problem. The shaded area is the feasible region, given by the constraint functions in black. The grey lines indicate contours of the objective function, with the unconstrained optimum at the grey dot. The central path of the problem is shown in red. $x^*(1)$ is marked by a red cross.\relax }}{27}{}\protected@file@percent }
\newlabel{fig:barrier_method_example}{{1.8}{27}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces The basic Barrier Method.}}{28}{}\protected@file@percent }
\newlabel{alg:product_axes}{{3}{28}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Nested Sampling with Barriers}{29}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Derivation}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Deriving the log barrier term}{30}{}\protected@file@percent }
\newlabel{eq:log_barrier_derivation_1}{{2.1}{30}}
\newlabel{eq:log_barrier_term}{{2.2}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Incorporating the log barrier term}{31}{}\protected@file@percent }
\abx@aux@cite{0}{collapsed_gibbs}
\abx@aux@segm{0}{0}{collapsed_gibbs}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualization of the unnormalized CDF $\Pi _q$ of $q$ for different values of $t$ and $q_{\textrm  {max}}=5$.\relax }}{34}{}\protected@file@percent }
\newlabel{fig:unnormalized}{{2.1}{34}}
\newlabel{eq:sub_zq_1}{{2.3}{35}}
\newlabel{eq:gamma_diff}{{2.4}{35}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Nested Sampling with the log barrier term.}}{36}{}\protected@file@percent }
\newlabel{alg:barriersampling}{{4}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of the normalized PDF (left) and normalized CDF (right) of $q$ for different values of $t$ and $q_{\textrm  {max}}=2$.\relax }}{37}{}\protected@file@percent }
\newlabel{fig:q_normalized}{{2.2}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Interpretation}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Interpreting $q$}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Influence of $t$ and $q_{\textrm  {max}}$}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Implementation of LRPS}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Rejection Sampling and Discretized LRPS}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Metropolis}{38}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A run of HMC for LRPS with a chain of five samples on a standard-normal-likelihood-restricted uniform prior space. The black dot is the starting spot, each blue cross is a generated sample. The background color shows the negative log density the particles move on, with lighter colors representing higher values.\relax }}{39}{}\protected@file@percent }
\newlabel{fig:barrier_sampling_hmc}{{2.3}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Hamiltonian Monte Carlo}{39}{}\protected@file@percent }
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Results}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Nested Sampling with and without Barriers}{40}{}\protected@file@percent }
\newlabel{fig:benchmark_examples}{{\caption@xref {fig:benchmark_examples}{ on input line 1387}}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Plot of the 2D example problem likelihoods with the slab-and-spike on the left and the nonconcentric Gaussians on the right.\relax }}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Influence of $t$ and $q_{\textrm  {max}}$}{42}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Summary}{43}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{45}{}\protected@file@percent }
\abx@aux@cite{0}{casellaberger}
\abx@aux@segm{0}{0}{casellaberger}
\@writefile{toc}{\contentsline {chapter}{Appendix}{46}{}\protected@file@percent }
\newlabel{eq:prior_mass_integral_appendix}{{3.1}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Visualization of the integral in (3.1\hbox {}). The shaded region is the integral, equivalent to $\mu (L^*) - \mu (\lambda )$.\relax }}{47}{}\protected@file@percent }
\newlabel{fig:vis_prior_masses_lrps_appendix}{{3.1}{47}}
\abx@aux@read@bbl@mdfivesum{10677C5FD58B2A8AE610A96B91D486D9}
\abx@aux@defaultrefcontext{0}{gibbs}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{metropolishastings}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hmc}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{slice_sampling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{annealed_importance_sampling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesian_cubature}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{skilling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hmc_in_ns}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{jaynes}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mcelreath}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mckay}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{betancourt_talk}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{liebloss}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{measure_theory}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{casellaberger}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{computational_stats}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nested_sampling_talk}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{automala}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{boyd}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{collapsed_gibbs}{none/global//global/global}
\gdef \@abspage@last{50}
