\relax 
\abx@aux@refcontext{none/global//global/global}
\abx@aux@cite{0}{gibbs}
\abx@aux@segm{0}{0}{gibbs}
\abx@aux@cite{0}{metropolis}
\abx@aux@segm{0}{0}{metropolis}
\abx@aux@cite{0}{hmc_origin}
\abx@aux@segm{0}{0}{hmc_origin}
\abx@aux@cite{0}{slice_sampling}
\abx@aux@segm{0}{0}{slice_sampling}
\abx@aux@cite{0}{annealed_importance_sampling}
\abx@aux@segm{0}{0}{annealed_importance_sampling}
\abx@aux@cite{0}{thermodynamic_integration}
\abx@aux@segm{0}{0}{thermodynamic_integration}
\abx@aux@cite{0}{bayesian_cubature}
\abx@aux@segm{0}{0}{bayesian_cubature}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{hmc_in_ns}
\abx@aux@segm{0}{0}{hmc_in_ns}
\@writefile{toc}{\contentsline {chapter}{Introduction}{5}{}\protected@file@percent }
\abx@aux@cite{0}{jaynes}
\abx@aux@segm{0}{0}{jaynes}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Background}{6}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Bayesian Statistics}{6}{}\protected@file@percent }
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\newlabel{eq:bayes}{{1.1}{7}}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\newlabel{eq:evidence}{{1.2}{8}}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Fitting Bayesian Models}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Analytical and Numerical Methods}{9}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Prior $pi(\theta ) = P(\theta )$, likelihood $L(\theta ) = P(H T H H H \,|\, \theta )$ and posterior $P(\theta \,|\, H T H H H)$ of $\theta $ in the coin example with $\pi (\theta ) = 2 \cdot (1-x)$ and data: H T H H H\relax }}{10}{}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:coin_example}{{1.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Monte Carlo Methods}{10}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Markov Chain Monte Carlo Methods}{11}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{mcelreath}
\abx@aux@segm{0}{0}{mcelreath}
\abx@aux@cite{0}{betancourt_talk}
\abx@aux@segm{0}{0}{betancourt_talk}
\abx@aux@cite{0}{bayesian_cubature}
\abx@aux@segm{0}{0}{bayesian_cubature}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{liebloss}
\abx@aux@segm{0}{0}{liebloss}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Nested Sampling}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}From a multi- to a one-dimensional Integral}{13}{}\protected@file@percent }
\abx@aux@cite{0}{measure_theory}
\abx@aux@segm{0}{0}{measure_theory}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Visualization of the relationship between a likelihood threshold $\lambda $ and the corresponding prior mass $\mu (\lambda )$ for a uniform prior (red) and a likelihood (blue).\relax }}{14}{}\protected@file@percent }
\newlabel{fig:xlambda}{{1.2}{14}}
\newlabel{eq:tonelli}{{1.3}{14}}
\newlabel{eq:integral_2}{{1.4}{14}}
\newlabel{eq:mu_definition}{{1.5}{14}}
\newlabel{eq:integral_3}{{1.6}{15}}
\newlabel{eq:integral_4}{{1.7}{15}}
\newlabel{eq:integral_5}{{1.8}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Sampling and Sorting}{15}{}\protected@file@percent }
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{measure_theory}
\abx@aux@segm{0}{0}{measure_theory}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Visualization of how the evidence integral can be approximated using prior mass samples $X_i$ sorted by their likelihoods, in this case using an upper sum. The shaded region is the approximate evidence. Based on a graphic in \blx@tocontentsinit {0}\cite [4]{skilling}.\relax }}{16}{}\protected@file@percent }
\newlabel{fig:integral_approximate}{{1.3}{16}}
\newlabel{eq:sample_ordering}{{1.9}{16}}
\newlabel{eq:approx_evidence}{{1.10}{16}}
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\@writefile{toc}{\contentsline {subsubsection}{Proof: Prior Masses of Prior Samples are Uniformly Distributed}{17}{}\protected@file@percent }
\newlabel{eq:L_integral}{{1.11}{17}}
\newlabel{eq:dirac_delta_property}{{1.12}{17}}
\abx@aux@cite{0}{measure_theory}
\abx@aux@segm{0}{0}{measure_theory}
\abx@aux@cite{0}{casellaberger}
\abx@aux@segm{0}{0}{casellaberger}
\abx@aux@cite{0}{computational_stats}
\abx@aux@segm{0}{0}{computational_stats}
\newlabel{eq:tonelli2}{{1.13}{18}}
\newlabel{eq:delta_property}{{1.14}{18}}
\newlabel{eq:L_cdf}{{1.15}{18}}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\abx@aux@cite{0}{beta_dist}
\abx@aux@segm{0}{0}{beta_dist}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Introducing a likelihood constraint}{19}{}\protected@file@percent }
\abx@aux@cite{0}{nested_sampling_talk}
\abx@aux@segm{0}{0}{nested_sampling_talk}
\abx@aux@cite{0}{kld}
\abx@aux@segm{0}{0}{kld}
\abx@aux@cite{0}{nested_sampling_talk}
\abx@aux@segm{0}{0}{nested_sampling_talk}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces The basic Nested Sampling algorithm.}}{21}{}\protected@file@percent }
\newlabel{alg:nested_sampling}{{1}{21}}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Intermediate states of two Nested Sampling runs on an eggcrate-shaped likelihood with a uniform prior. Black crosses are dead points, red crosses are live points. All live points are inside the current likelihood constraint contour. The likelihood is visualized by the colored background, lighter colors represent higher likelihood values. The left image shows an earlier iteration than the right one, as can be seen by the lower number of dead points and the looser likelihood constraint contours.\relax }}{22}{}\protected@file@percent }
\newlabel{fig:ns_eggcrate_example}{{1.4}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Likelihood-restricted prior sampling}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Rejection Sampling}{22}{}\protected@file@percent }
\abx@aux@cite{0}{mckay}
\abx@aux@segm{0}{0}{mckay}
\abx@aux@cite{0}{automala}
\abx@aux@segm{0}{0}{automala}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Discretized LRPS}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Metropolis Algorithm}{23}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Likelihood-restricted prior sampling using Metropolis.}}{23}{}\protected@file@percent }
\newlabel{alg:lrpsmetropolis}{{2}{23}}
\abx@aux@cite{0}{hmc}
\abx@aux@segm{0}{0}{hmc}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Visualization of the Metropolis algorithm in a two-dimensional sample space. The dotted line indicates the likelihood constraint boundary, the dot is the starting point and each cross is a sampled point.\relax }}{24}{}\protected@file@percent }
\newlabel{fig:metropolis_example}{{1.5}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Hamiltonian Monte Carlo}{24}{}\protected@file@percent }
\abx@aux@cite{0}{hmc_in_ns}
\abx@aux@segm{0}{0}{hmc_in_ns}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Visualization of HMC in a two-dimensional sample space. Contour lines and shading indicate the negative log probability of the target distribution (a mixture of two Gaussians). The dot is the starting point and each cross is a sampled point.\relax }}{25}{}\protected@file@percent }
\newlabel{fig:hmc_example}{{1.6}{25}}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces Left: An iteration of HMC for LRPS with reflection and a chain of five samples on a standard-normal-likelihood-restricted uniform prior space. The black dot is the starting spot, each blue cross is a generated sample. Right: A later iteration of the same example. Note how every integration step contains a reflection and how far away the reflection points are from the actual boundary.\relax }}{26}{}\protected@file@percent }
\newlabel{fig:hmc_reflection_example}{{1.7}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}The Barrier Method}{26}{}\protected@file@percent }
\newlabel{eq:cvxproblem}{{1.16}{26}}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\newlabel{eq:cvxproblem_unconstrained}{{1.17}{27}}
\newlabel{eq:exact_barrier_function}{{1.18}{27}}
\newlabel{eq:log_barrier_function}{{1.19}{27}}
\newlabel{eq:barrierproblem}{{1.20}{27}}
\newlabel{eq:objective}{{1.21}{27}}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\abx@aux@cite{0}{boyd}
\abx@aux@segm{0}{0}{boyd}
\@writefile{lof}{\contentsline {figure}{\numberline {1.8}{\ignorespaces Visualization of the exact barrier function $I$ in blue, and three log barrier functions (with $t=1,4,16$) in red. The log barrier becomes a better approximation as $t$ increases.\relax }}{28}{}\protected@file@percent }
\newlabel{fig:logbarrier}{{1.8}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.9}{\ignorespaces Central path of an example optimization problem. The shaded area is the feasible region, given by the constraint functions in black. The gray lines indicate contours of the objective function, with the unconstrained optimum at the gray dot. The central path of the problem is shown in red. $x^*(1)$ is marked by a red cross.\relax }}{29}{}\protected@file@percent }
\newlabel{fig:barrier_method_example}{{1.9}{29}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces The basic Barrier Method.}}{29}{}\protected@file@percent }
\newlabel{alg:barrier_method}{{3}{29}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Nested Sampling with Barriers}{30}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Motivation}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Derivation}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Deriving the log barrier term}{31}{}\protected@file@percent }
\newlabel{eq:last_optimization_problem}{{2.1}{32}}
\newlabel{eq:log_barrier_term}{{2.2}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Incorporating the log barrier term}{32}{}\protected@file@percent }
\abx@aux@cite{0}{collapsed_gibbs}
\abx@aux@segm{0}{0}{collapsed_gibbs}
\newlabel{eq:recognize_bayes}{{2.3}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualization of the unnormalized CDF $\Pi _q$ of $q$ for different values of $t$ and $q_{\textrm  {max}}=5$.\relax }}{35}{}\protected@file@percent }
\newlabel{fig:unnormalized}{{2.1}{35}}
\abx@aux@cite{0}{bayesian_data_analysis}
\abx@aux@segm{0}{0}{bayesian_data_analysis}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Nested Sampling with the log barrier term.}}{36}{}\protected@file@percent }
\newlabel{alg:barriersampling}{{4}{36}}
\newlabel{eq:sub_zq_1}{{2.4}{37}}
\newlabel{eq:gamma_diff}{{2.5}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Interpretation}{37}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of the normalized PDF (left) and normalized CDF (right) of $q$ for different values of $t$ and $q_{\textrm  {max}}=2$.\relax }}{38}{}\protected@file@percent }
\newlabel{fig:q_normalized}{{2.2}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Interpreting $q$}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Influence of $t$ and $q_{\textrm  {max}}$}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Implementation of LRPS}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Rejection Sampling and Discretized LRPS}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Metropolis Algorithm}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Hamiltonian Monte Carlo}{39}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A run of HMC for LRPS with a chain of five samples on a standard-normal-likelihood-restricted uniform prior space. The black dot is the starting spot, each black cross is a generated sample. The background color shows the negative log density the particles move on, with lighter colors representing higher values.\relax }}{40}{}\protected@file@percent }
\newlabel{fig:barrier_sampling_hmc}{{2.3}{40}}
\abx@aux@cite{0}{skilling}
\abx@aux@segm{0}{0}{skilling}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Results}{41}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Nested Sampling with and without Barriers}{41}{}\protected@file@percent }
\abx@aux@cite{0}{sphere_sampling}
\abx@aux@segm{0}{0}{sphere_sampling}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Estimated Evidence}{42}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Box plot of the deviation of each method's evidence estimate's from the true value. Top: $L_1$-example, Bottom: $L_2$-example. Orange lines are the median values, gray crosses are the mean values.\relax }}{43}{}\protected@file@percent }
\newlabel{fig:results_logZ_diffs}{{3.1}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Posterior Modes Found}{43}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Box plots of how many of the two modes were found by each method. The values represent $\frac  {m}{2}$, where $m$ is the number of modes found. Top: $L_1$-example, Bottom: $L_2$-example. Orange lines are the median values, gray crosses are the mean values.\relax }}{44}{}\protected@file@percent }
\newlabel{fig:results_modes_found}{{3.2}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Number of Iterations}{44}{}\protected@file@percent }
\abx@aux@cite{0}{rjd}
\abx@aux@segm{0}{0}{rjd}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Box plots of the KLD of estimated and true mode weights. Top: $L_1$-example, Bottom: $L_2$-example. The results for HMC with reflection are presented on a separate scale to enable better comparison between the others. Orange lines are the median values, gray crosses are the mean values.\relax }}{45}{}\protected@file@percent }
\newlabel{fig:results_KLDs}{{3.3}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Acceptance Rate}{45}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Box plots of how many iterations each method took to reach the stopping point. Top: $L_1$-example, Bottom: $L_2$-example. Orange lines are the median values, gray crosses are the mean values.\relax }}{46}{}\protected@file@percent }
\newlabel{fig:results_iterations}{{3.4}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Box plots of the ratio of accepted proposals to total proposals made during LRPS for each method. Top: $L_1$-example, Bottom: $L_2$-example. Orange lines are the median values, gray crosses are the mean values.\relax }}{46}{}\protected@file@percent }
\newlabel{fig:results_acceptance_rates}{{3.5}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Mean jump distance of each configuration per iteration; Unweighted rolling average with a window of 50. Top: $L_1$-example, Bottom: $L_2$-example. Curves are only shown as far as the shortest run went.\relax }}{47}{}\protected@file@percent }
\newlabel{figs:results_jump_distances}{{3.6}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Jump Distance}{47}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Box plot of each configuration's evidence estimate's deviation from the true value. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{48}{}\protected@file@percent }
\newlabel{fig:results_params_logZ}{{3.7}{48}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Influence of $t$ and $q_{\textrm  {max}}$}{48}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Estimated Evidence}{48}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Box plots of how many of the two modes were found by each method. The values represent $\frac  {m}{2}$, where $m$ is the number of modes found. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{49}{}\protected@file@percent }
\newlabel{fig:results_params_modes_found}{{3.8}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Posterior Modes Found}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Number of Iterations}{49}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Box plots of the KLD of estimated and true mode weights. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{50}{}\protected@file@percent }
\newlabel{fig:results_params_KLDs}{{3.9}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Box plots of how many iterations each configuration took to reach the stopping point. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{50}{}\protected@file@percent }
\newlabel{fig:results_params_iterations}{{3.10}{50}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Box plots of the ratio of accepted proposals to total proposals made during LRPS for each configuration. Top: Using Metropolis for LPRS, Bottom: Using HMC for LRPS. Orange lines are the median values, gray crosses are the mean values.\relax }}{51}{}\protected@file@percent }
\newlabel{fig:results_params_acceptance_rates}{{3.11}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Acceptance Rate}{51}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Jump Distance}{51}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Mean jump distance of each Metropolis configuration per iteration.\relax }}{52}{}\protected@file@percent }
\newlabel{fig:results_params_jump_distances_metropolis}{{3.12}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Mean jump distance of each HMC configuration per iteration.\relax }}{52}{}\protected@file@percent }
\newlabel{fig:results_params_jump_distances_hmc}{{3.13}{52}}
\abx@aux@cite{0}{ultranest}
\abx@aux@segm{0}{0}{ultranest}
\abx@aux@cite{0}{dynesty}
\abx@aux@segm{0}{0}{dynesty}
\abx@aux@cite{0}{jaxns}
\abx@aux@segm{0}{0}{jaxns}
\abx@aux@cite{0}{polychord}
\abx@aux@segm{0}{0}{polychord}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Conclusions}{53}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{55}{}\protected@file@percent }
\abx@aux@cite{0}{casellaberger}
\abx@aux@segm{0}{0}{casellaberger}
\@writefile{toc}{\contentsline {chapter}{Appendix}{58}{}\protected@file@percent }
\newlabel{eq:prior_mass_integral_appendix}{{4.1}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Visualization of the integral in (4.1\hbox {}). The shaded region is the integral, equivalent to $\mu (L^*) - \mu (\lambda )$.\relax }}{59}{}\protected@file@percent }
\newlabel{fig:vis_prior_masses_lrps_appendix}{{4.1}{59}}
\abx@aux@read@bbl@mdfivesum{03BF60F5A16292CF05C3DE0BDB0EDEB9}
\abx@aux@defaultrefcontext{0}{gibbs}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{metropolis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hmc_origin}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{slice_sampling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{annealed_importance_sampling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{thermodynamic_integration}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesian_cubature}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{skilling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hmc_in_ns}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{jaynes}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mcelreath}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{mckay}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{betancourt_talk}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{liebloss}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{measure_theory}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{casellaberger}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{computational_stats}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{beta_dist}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nested_sampling_talk}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kld}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{automala}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{hmc}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{boyd}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{collapsed_gibbs}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bayesian_data_analysis}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{sphere_sampling}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rjd}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ultranest}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{dynesty}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{jaxns}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{polychord}{none/global//global/global}
\gdef \@abspage@last{63}
