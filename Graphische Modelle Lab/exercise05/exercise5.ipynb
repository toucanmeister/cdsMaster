{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5435c02",
   "metadata": {},
   "source": [
    "# Exercise 5 - Multivariate Gaussians\n",
    "\n",
    "In this exercise, we will estimate a Gaussian from a dataset and answer inference queries using the mean- and canonical parameterizations. Runtime experiments will illustrate the importance of both parameterizations.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        04.12.2022\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=34630)\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa48c929",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec52b2",
   "metadata": {},
   "source": [
    "In this exercise, we will use a dataset used for [predicting wine quality](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009).\n",
    "\n",
    "You find this dataset stored as `dataset.csv`. \n",
    "\n",
    "### Task 1\n",
    "Read this dataset into a $1599\\times 12$ matrix.\n",
    "\n",
    "Each row represents one specific wine, each column corresponds to a measured attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02642fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0153a796-79dd-4b21-a322-85173a980c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('dataset.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07097c76",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "Here we use the model assumption that the samples come from a multivariate normal distribution. \n",
    "\n",
    "### Task 2\n",
    "Estimate the Maximum Likelihood parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_{\\text{ML}} &= \\frac{1}{N}\\sum_{i=1}^Nx^{(i)}\\\\\n",
    "\\Sigma_{\\text{ML}} &= \\frac{1}{N}\\sum_{i=1}^N\\left(x^{(i)}-\\mu_{\\text{ML}}\\right)\\left(x^{(i)}-\\mu_{\\text{ML}}\\right)^T\n",
    "\\end{align}\n",
    "\n",
    "for a multivariate normal distribution based on this dataset. Here $N$ is the number of samples and $x^{(i)}$ is the i-th sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84a7b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.02952057e+00, -7.98014785e-02,  2.27677527e-01,\n",
       "         2.81580055e-01,  7.67389030e-03, -2.79916982e+00,\n",
       "        -6.47829186e+00,  2.19385070e-03, -1.83470891e-01,\n",
       "         5.39763142e-02, -1.14349595e-01,  1.74314505e-01],\n",
       "       [-7.98014785e-02,  3.20423261e-02, -1.92595685e-02,\n",
       "         4.83888167e-04,  5.16263851e-04, -1.96612867e-02,\n",
       "         4.50144000e-01,  7.43900996e-06,  6.49063758e-03,\n",
       "        -7.91647985e-03, -3.85760812e-02, -5.64405638e-02],\n",
       "       [ 2.27677527e-01, -1.92595685e-02,  3.79237511e-02,\n",
       "         3.94096081e-02,  1.86755609e-03, -1.24174408e-01,\n",
       "         2.27554874e-01,  1.34090669e-04, -1.62873900e-02,\n",
       "         1.03212557e-02,  2.28009045e-02,  3.55896216e-02],\n",
       "       [ 2.81580055e-01,  4.83888167e-04,  3.94096081e-02,\n",
       "         1.98665392e+00,  3.68786810e-03,  2.75688624e+00,\n",
       "         9.41055252e+00,  9.44819611e-04, -1.86326290e-02,\n",
       "         1.32011525e-03,  6.31794232e-02,  1.56252677e-02],\n",
       "       [ 7.67389030e-03,  5.16263851e-04,  1.86755609e-03,\n",
       "         3.68786810e-03,  2.21375732e-03,  2.73659057e-03,\n",
       "         7.33408548e-02,  1.78106112e-05, -1.92454062e-03,\n",
       "         2.96002562e-03, -1.10845812e-02, -4.89648080e-03],\n",
       "       [-2.79916982e+00, -1.96612867e-02, -1.24174408e-01,\n",
       "         2.75688624e+00,  2.73659057e-03,  1.09346457e+02,\n",
       "         2.29593845e+02, -4.32979465e-04,  1.13582013e-01,\n",
       "         9.15351899e-02, -7.73214536e-01, -4.27639460e-01],\n",
       "       [-6.47829186e+00,  4.50144000e-01,  2.27554874e-01,\n",
       "         9.41055252e+00,  7.33408548e-02,  2.29593845e+02,\n",
       "         1.08142564e+03,  4.42195996e-03, -3.37487599e-01,\n",
       "         2.39321242e-01, -7.20478927e+00, -4.91416188e+00],\n",
       "       [ 2.19385070e-03,  7.43900996e-06,  1.34090669e-04,\n",
       "         9.44819611e-04,  1.78106112e-05, -4.32979465e-04,\n",
       "         4.42195996e-03,  3.55980179e-06, -9.95016816e-05,\n",
       "         4.74799064e-05, -9.97327680e-04, -2.66436973e-04],\n",
       "       [-1.83470891e-01,  6.49063758e-03, -1.62873900e-02,\n",
       "        -1.86326290e-02, -1.92454062e-03,  1.13582013e-01,\n",
       "        -3.37487599e-01, -9.95016816e-05,  2.38202742e-02,\n",
       "        -5.14296744e-03,  3.38104587e-02, -7.19332087e-03],\n",
       "       [ 5.39763142e-02, -7.91647985e-03,  1.03212557e-02,\n",
       "         1.32011525e-03,  2.96002562e-03,  9.15351899e-02,\n",
       "         2.39321242e-01,  4.74799064e-05, -5.14296744e-03,\n",
       "         2.87146470e-02,  1.68962039e-02,  3.43918866e-02],\n",
       "       [-1.14349595e-01, -3.85760812e-02,  2.28009045e-02,\n",
       "         6.31794232e-02, -1.10845812e-02, -7.73214536e-01,\n",
       "        -7.20478927e+00, -9.97327680e-04,  3.38104587e-02,\n",
       "         1.68962039e-02,  1.13493717e+00,  4.09532733e-01],\n",
       "       [ 1.74314505e-01, -5.64405638e-02,  3.55896216e-02,\n",
       "         1.56252677e-02, -4.89648080e-03, -4.27639460e-01,\n",
       "        -4.91416188e+00, -2.66436973e-04, -7.19332087e-03,\n",
       "         3.43918866e-02,  4.09532733e-01,  6.51760540e-01]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = X.shape[0]\n",
    "sample_mean = (1/N) * np.sum(X, axis=0)\n",
    "M = X.shape[1]\n",
    "sample_cov = np.zeros((M,M))\n",
    "for i in range(N):\n",
    "    sample_cov += np.outer(X[i]-sample_mean, X[i]-sample_mean)\n",
    "sample_cov = sample_cov / N\n",
    "sample_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48177fa",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Now that we have estimated the parameters of our underlying model, we want to perform inference in order to answer the query:\n",
    "\n",
    "**\"What quality and alcohol level can we expect, if we observe a wine with**\n",
    "- **citric acid level of 0.6,**\n",
    "- **residual sugar of 2.5,**\n",
    "- **chlorides level of 0.1,**\n",
    "- **density of 0.994,**\n",
    "- **sulphate level of 0.5?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213711cb",
   "metadata": {},
   "source": [
    "## Mean Parameterization\n",
    "\n",
    "The mean parameterization of a Gaussian consists of the mean vector $\\mu$ and the covariance matrix $\\Sigma$.\n",
    "\n",
    "**Marginalizing** dimensions from a Gaussian, to keep a subset $J$ of the dimensions results in a Gaussian with \n",
    "- Mean vector $\\mu_J$\n",
    "- Covariance matrix $\\Sigma_{JJ}$\n",
    "\n",
    "**Conditioning** a subset $J$ of the dimensions on values $x_J$ also gives us a Gaussian with \n",
    "- Mean vector $\\mu_I+\\Sigma_{IJ}\\Sigma_{JJ}^{-1}(x_J-\\mu_J)$ \n",
    "- Covariance matrix $S_{II} = \\Sigma_{II}-\\Sigma_{IJ}\\Sigma_{JJ}^{-1}\\Sigma_{JI}$\n",
    "\n",
    "Here, the subscripts indicate the selected dimensions of the variables. $I$ denotes the remaining dimensions, after we condition on the dimensions $J$. $S$ denotes the Schur complement.\n",
    "\n",
    "### Task 3\n",
    "Implement the following class of a Gaussian with mean parameterization. Then use your implementation to answer the query.\n",
    "\n",
    "Note: `marginalize` and `condition` should not return any parameters, but update the internal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "922ae260-68af-4cdb-ba7d-fbc607778ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myindex(arr, idx_I, idx_J=[]): # Leaves only the indices idx_I and idx_J in array arr\n",
    "    if arr.ndim > 2: return arr\n",
    "    if arr.ndim == 1: return arr[idx_I]\n",
    "    if arr.ndim == 2: return arr[idx_I,:][:,idx_J]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c17f26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.78770805,  6.10114144])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MeanGaussian():\n",
    "    def __init__(self, mu, sigma):\n",
    "        '''\n",
    "        Mean parameterization of a gaussian\n",
    "        \n",
    "        @Params: \n",
    "            mu... vector of size ndims\n",
    "            sigma... matrix of size ndims x ndims\n",
    "        '''\n",
    "        \n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def marginalize(self,idx_J):\n",
    "        '''\n",
    "        Marginalizes a set of indices from the Gaussian.\n",
    "    \n",
    "        @Params:\n",
    "            idx_J... list of indices to keep after marginalization (these indices remain)\n",
    "            \n",
    "        @Returns:\n",
    "            Nothing, parameters are changed internally\n",
    "        '''\n",
    "        self.mu = self.mu[idx_J] # mu and sigma have to be numpy arrays\n",
    "        self.sigma = myindex(self.sigma, idx_J, idx_J)\n",
    "        \n",
    "    \n",
    "    def condition(self,idx_J, x_J):\n",
    "        '''\n",
    "        Conditions a set of indices on values.\n",
    "        \n",
    "        @Params:\n",
    "            idx_J... list of indices that are conditioned on\n",
    "            x_J... values that are conditioned on\n",
    "            \n",
    "        @Returns:\n",
    "            Nothing, parameters are changed internally\n",
    "        '''\n",
    "        idx_I = np.array([idx for idx in range(len(self.mu)) if idx not in idx_J])\n",
    "        sigma_ii = myindex(self.sigma, idx_I, idx_I)\n",
    "        sigma_ij = myindex(self.sigma, idx_I, idx_J)\n",
    "        sigma_ji = myindex(self.sigma, idx_J, idx_I)\n",
    "        sigma_jj = myindex(self.sigma, idx_J, idx_J)\n",
    "        self.mu = self.mu[idx_I] + sigma_ij @ np.linalg.inv(sigma_jj) @ (x_J - self.mu[idx_J])\n",
    "        self.sigma = sigma_ii - sigma_ij @ np.linalg.inv(sigma_jj) @ sigma_ji\n",
    "\n",
    "distribution = MeanGaussian(sample_mean, sample_cov)\n",
    "idx_J = [2,3,4,7,9]\n",
    "x_J = [0.6, 2.5, 0.1, 0.994, 0.5]\n",
    "distribution.condition(idx_J, x_J)\n",
    "distribution.marginalize([5,6])\n",
    "distribution.mu\n",
    "# Mu gives us the expected values alcohol and quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71f4d5",
   "metadata": {},
   "source": [
    "## Canonical Parameterization\n",
    "\n",
    "The canonical parameterization $(\\nu,\\Lambda)$ results from the mean parameterization trough\n",
    "\n",
    "\\begin{align}\n",
    "\\nu &=\\Sigma^{-1}\\mu\\\\\n",
    "\\Lambda &= \\Sigma^{-1}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "In the canonical parameterization, **marginalizing** dimensions from a Gaussian, to keep a subset $J$ of the dimensions, results in a Gaussian with \n",
    "- Vector $\\nu_J-\\Lambda_{JI}\\Lambda_{II}^{-1}\\nu_I$\n",
    "- Precision matrix $S_{JJ}=\\Lambda_{JJ}-\\Lambda_{JI}\\Lambda_{II}^{-1}\\Lambda_{IJ}$\n",
    "\n",
    "**Conditioning** a subset $J$ of the dimensions on values $x_J$ again gives us a Gaussian with \n",
    "- Vector $\\nu_I-\\Lambda_{IJ}x_J$\n",
    "- Precision matrix $\\Lambda_{II}$\n",
    "\n",
    "The subscripts indicate the selected dimensions of the variables. $I$ denotes the remaining dimensions, after we remove the dimensions $J$. $S$ denotes the Schur complement.\n",
    "\n",
    "We shall later see, that there are some cases, where you would prefer canonical parameterization over the mean parameterization.\n",
    "\n",
    "### Task 4\n",
    "Implement the following class of a Gaussian with canonical parameterization. Then use your implementation to answer the query.\n",
    "\n",
    "Note: `marginalize` and `condition` should not return any parameters, but update the internal parameters.\n",
    "The solution should be the same as in Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f78821f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.78770805,  6.10114144])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CanonicalGaussian():\n",
    "    def __init__(self, nu, lamb):\n",
    "        '''\n",
    "        Canconical representation of a gaussian\n",
    "        \n",
    "        @Params: \n",
    "            nu... vector of size ndims\n",
    "            lamb... matrix of size ndims x ndims (precision matrix)\n",
    "        '''\n",
    "        \n",
    "        self.nu = nu\n",
    "        self.lamb = lamb\n",
    "        \n",
    "    def marginalize(self,idx_J):\n",
    "        '''\n",
    "        Marginalizes a set of indices from the Gaussian.\n",
    "    \n",
    "        @Params:\n",
    "            idx_J... list of indices to keep after marginalization (these indices remain)\n",
    "            \n",
    "        @Returns:\n",
    "            Nothing, parameters are changed internally\n",
    "        '''\n",
    "        \n",
    "        idx_I = np.array([idx for idx in range(len(self.nu)) if idx not in idx_J])\n",
    "        lambda_ii = myindex(self.lamb, idx_I, idx_I)\n",
    "        lambda_ij = myindex(self.lamb, idx_I, idx_J)\n",
    "        lambda_ji = myindex(self.lamb, idx_J, idx_I)\n",
    "        lambda_jj = myindex(self.lamb, idx_J, idx_J)\n",
    "        self.nu = self.nu[idx_J] - lambda_ji @ np.linalg.inv(lambda_ii) @ self.nu[idx_I]\n",
    "        self.lamb = lambda_jj - lambda_ji @ np.linalg.inv(lambda_ii) @ lambda_ij\n",
    "        \n",
    "    \n",
    "    def condition(self,idx_J, x_J):\n",
    "        '''\n",
    "        Conditions a set of indices on values.\n",
    "        \n",
    "        @Params:\n",
    "            idx_J... list of indices that are conditioned on\n",
    "            x_J... values that are conditioned on\n",
    "            \n",
    "        @Returns:\n",
    "            Nothing, parameters are changed internally\n",
    "        '''\n",
    "        idx_I = np.array([idx for idx in range(len(self.nu)) if idx not in idx_J])\n",
    "        lambda_ii = myindex(self.lamb, idx_I, idx_I)\n",
    "        lambda_ij = myindex(self.lamb, idx_I, idx_J)\n",
    "        self.nu = self.nu[idx_I] - lambda_ij @ x_J\n",
    "        self.lamb = lambda_ii\n",
    "    \n",
    "distribution = CanonicalGaussian(np.linalg.inv(sample_cov) @ sample_mean, np.linalg.inv(sample_cov))\n",
    "idx_J = [2,3,4,7,9]\n",
    "x_J = [0.6, 2.5, 0.1, 0.994, 0.5]\n",
    "distribution.condition(idx_J, x_J)\n",
    "distribution.marginalize([5,6])\n",
    "\n",
    "# nu = inv(sigma) @ mu\n",
    "# sigma @ nu = mu\n",
    "# inv(lambda) @ nu = mu\n",
    "np.linalg.inv(distribution.lamb) @ distribution.nu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d226f",
   "metadata": {},
   "source": [
    "# Computational costs\n",
    "\n",
    "Why do we need two different parameterizations of the same probability distribution?\n",
    "What is the difference?\n",
    "\n",
    "We cannot observe the effect of a different parameterization on our dataset, as it is way to small (too few dimensions).\n",
    "\n",
    "In the `synthetic/` directory, you find parameters for a Gaussian with 300 dimensions, as well as a value vector `x` for conditioning.\n",
    "Load these arrays and calculate the parameters for the canoncial parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31ed39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.load('synthetic/mu.npy')\n",
    "sigma = np.load('synthetic/sigma.npy')\n",
    "x = np.load('synthetic/x.npy')\n",
    "lamb = np.linalg.inv(sigma)\n",
    "nu = lamb @ mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70f527",
   "metadata": {},
   "source": [
    "We now want to investigate the computation times for the following inference operations:\n",
    "\n",
    "1. Marginalize out the dimensions 200-299, then condition on the dimensions 100-199 with $x$\n",
    "2. Condition on the dimensions 100-199 with $x$, then marginalize out the dimensions 200-299\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"images/indices.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "Both operations yield the same result, $p(x_0,\\dots,x_{99}|x_{100},\\dots,x_{199})$ they just change the order of marginalization and conditioning.\n",
    "\n",
    "### Task 5\n",
    "Track the computational costs for both inference operations using the mean parameters and the canoncial parameters.\n",
    "\n",
    "What do you observe? Try to find an explanation for your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a95a9b08-3a12-4cee-a174-28b9b068e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "meang = MeanGaussian(mu, sigma)\n",
    "canonicalg = CanonicalGaussian(nu, lamb)\n",
    "\n",
    "# 1.\n",
    "start = time.time()\n",
    "meang.marginalize([x for x in range(200)])\n",
    "meang.condition([x for x in range(100,200)], x)\n",
    "end = time.time()\n",
    "mean_1 = end-start\n",
    "\n",
    "start = time.time()\n",
    "canonicalg.marginalize([x for x in range(200)])\n",
    "canonicalg.condition([x for x in range(100,200)], x)\n",
    "end = time.time()\n",
    "canonical_1 = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f6be65-4ef4-4af1-8e56-de592689b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "meang = MeanGaussian(mu, sigma)\n",
    "canonicalg = CanonicalGaussian(nu, lamb)\n",
    "\n",
    "# 2.\n",
    "start = time.time()\n",
    "meang.condition([x for x in range(100,200)], x)\n",
    "meang.marginalize([x for x in range(100)])\n",
    "end = time.time()\n",
    "mean_2 = end-start\n",
    "\n",
    "start = time.time()\n",
    "canonicalg.condition([x for x in range(100,200)], x)\n",
    "canonicalg.marginalize([x for x in range(100)])\n",
    "end = time.time()\n",
    "canonical_2 = end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6e2ed54-e66b-42bf-8c8f-aa4a32635705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Mean                 Canonical\n",
      "1: 0.008557558059692383 0.01397848129272461\n",
      "2: 0.013402938842773438 0.007674455642700195\n"
     ]
    }
   ],
   "source": [
    "print('   Mean                 Canonical')\n",
    "print(f'1: {mean_1} {canonical_1}')\n",
    "print(f'2: {mean_2} {canonical_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80de120-aa4d-4384-8e42-6899ee82740b",
   "metadata": {},
   "source": [
    "Mean parameterization is about twice as fast as canonical parameterization when we marginalize first, and canonical parameterization is about twice as fast as mean parameterization when we condition first.\n",
    "\n",
    "I would think that the speed differences come from the fact for mean parameterization, marginalization is a cheaper computation than marginalization, while it is the other way around for canonical parameterization. This causes the cases where the cheap computation is done first (being done on the larger parameters, compared to the computation done second), to go faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
