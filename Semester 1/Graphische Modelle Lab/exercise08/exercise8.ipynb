{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6eb788",
   "metadata": {},
   "source": [
    "# Exercise 8 - Basic Samplers\n",
    "\n",
    "In this exercise, we will build samplers to generate samples from categoricals and gaussians.\n",
    "\n",
    "In the event of a persistent problem, do not hesitate to contact the course instructors under\n",
    "- paul.kahlmeyer@uni-jena.de\n",
    "\n",
    "### Submission\n",
    "\n",
    "- Deadline of submission:\n",
    "        08.01.2023\n",
    "- Submission on [moodle page](https://moodle.uni-jena.de/course/view.php?id=34630)\n",
    "\n",
    "### Help\n",
    "In case you cannot solve a task, you can use the saved values within the `help` directory:\n",
    "- Load arrays with [Numpy](https://numpy.org/doc/stable/reference/generated/numpy.load.html)\n",
    "```\n",
    "np.load('help/array_name.npy')\n",
    "```\n",
    "- Load functions, classes and other objects with [Dill](https://dill.readthedocs.io/en/latest/dill.html)\n",
    "```\n",
    "import dill\n",
    "with open('help/some_func.pkl', 'rb') as f:\n",
    "    func = dill.load(f)\n",
    "```\n",
    "\n",
    "to continue working on the other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e2749",
   "metadata": {},
   "source": [
    "# Sampling from Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24a66c",
   "metadata": {},
   "source": [
    "Sampling from a distribution is mostly done by mapping the unit interval $[0, 1]$ onto the sampling space of the distribution. If we then have a sampler on the unit interval, we can map these samples onto the target sample space.\n",
    "\n",
    "How to sample from $[0,1]$ was the content of the last exercise. In this exercise we will leave sampling from $[0, 1]$ to [Numpy](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc33ad0",
   "metadata": {},
   "source": [
    "## Categorical\n",
    "\n",
    "Transforming samples from $[0, 1]$ into categories of a categorical with probability vector $p$ is done with **Inverse Transform Sampling**:\n",
    "\n",
    "We split the interval into bins of size $p(x=i)$ for each category $i$.\n",
    "Then we can assign each $u\\in[0, 1]$ to the category of the bin it falls into. \n",
    "\n",
    "The following figure illustrates this process for 4 categories. The blue balls are samples in $[0, 1]$ and fall into bins that are of the width of the respective category. \n",
    "<div>\n",
    "<img src=\"images/its.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Implement the inverse transform sampling for categoricals.\n",
    "\n",
    "Use this function to sample from a categorical with \n",
    "\\begin{equation}\n",
    "p = [0.1, 0.2 , 0.1, 0.15 , 0.13, 0.32]^T\n",
    "\\end{equation}\n",
    "\n",
    "and compare the true $p$ to the maximum likelihood estimate based on an increasing number of samples over 10 tries.\n",
    "\n",
    "For visualization you can use the calculation of a mean confidence interval in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "593a7b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True p: [0.1  0.2  0.1  0.15 0.13 0.32]\n",
      "Estimated p with 5 samples: [0.  0.4 0.  0.4 0.  0.2]\n",
      "Estimated p with 10 samples: [0.  0.1 0.2 0.  0.2 0.5]\n",
      "Estimated p with 100 samples: [0.14 0.22 0.03 0.13 0.14 0.34]\n",
      "Estimated p with 500 samples: [0.112 0.2   0.09  0.142 0.158 0.298]\n",
      "Estimated p with 1000 samples: [0.09  0.2   0.094 0.178 0.117 0.321]\n",
      "Estimated p with 5000 samples: [0.1024 0.1904 0.1032 0.152  0.132  0.32  ]\n",
      "Estimated p with 10000 samples: [0.1055 0.1984 0.0906 0.1557 0.1297 0.3201]\n",
      "Estimated p with 50000 samples: [0.09784 0.20014 0.10148 0.14954 0.13258 0.31842]\n",
      "Estimated p with 100000 samples: [0.09878 0.19987 0.09976 0.15067 0.12932 0.3216 ]\n",
      "Estimated p with 1000000 samples: [0.100032 0.200124 0.099985 0.149784 0.130162 0.319913]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "def sample_categorical(p: np.ndarray, N:int = 1) -> np.ndarray:\n",
    "    '''\n",
    "    Samples from a categorical. \n",
    "    \n",
    "    @Params:\n",
    "        p... probability vector\n",
    "        N... number of samples\n",
    "        \n",
    "    @Returns:\n",
    "        samples from a categorical distribution\n",
    "    '''\n",
    "    p_cumulative = np.array([p[0:k].sum() for k in range(1,p.size+1)])\n",
    "    unit_samples = np.random.random_sample(N)\n",
    "    samples = []\n",
    "    for unit_sample in unit_samples:\n",
    "        for idx,breakpoint in enumerate(p_cumulative):\n",
    "            if unit_sample <= breakpoint: \n",
    "                samples.append(idx)\n",
    "                break\n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "p_true = np.array([0.1, 0.2 , 0.1, 0.15 , 0.13, 0.32])\n",
    "num_samples = [5, 10, 100, 500, 1000, 5000, 10000, 50000, 100000, 1000000]\n",
    "print(f'True p: {p_true}')\n",
    "for n in num_samples:\n",
    "    samples = sample_categorical(p_true, n)\n",
    "    p_estimate = np.histogram(samples, range(6+1))[0] / n\n",
    "    print(f'Estimated p with {n} samples: {p_estimate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0e8700",
   "metadata": {},
   "source": [
    "## Standard Normal Distribution\n",
    "\n",
    "The  [Box Muller Transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) is a method to transform samples $u, v \\in[0, 1]^2$ into samples $x_0, x_1$ from a standard normal distribution $\\mathcal{N}(0, 1)$. \n",
    "\n",
    "This is done by treating $u$ and $v$ as probabilities for sampling a point $x_1, x_2$ of a bivariate standard normal distribution with an angle $\\theta$ and a distance $r$ from the origin. \n",
    "\n",
    "<div>\n",
    "<img src=\"images/boxmuller.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "We transform $u$ and $v$ to actual $\\hat{\\theta}$ and $\\hat{r}$ by using the [inverse cumulative density function](https://en.wikipedia.org/wiki/Quantile_function):\n",
    "\n",
    "1. The angle $\\theta$ is uniformly distributed in $[0, 2\\pi]$.\\\n",
    "We transform $u$ to $\\hat{\\theta}$ using  $p\\left(\\theta < \\hat{\\theta}\\right) = \\text{cdf}(\\hat{\\theta})= u$.\n",
    "2. The half squared distance $\\frac{r^2}{2}$ from the origin is exponentially distributed with $\\lambda = 1$.\\\n",
    "We transform $v$ to $\\frac{\\hat{r}^2}{2}$ using $p\\left(\\frac{r^2}{2} < \\frac{\\hat{r}^2}{2}\\right) = \\text{cdf}\\left(\\frac{\\hat{r}^2}{2}\\right)= v$\n",
    "\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Derive the formulas to calculate $\\hat{\\theta}$ and $\\hat{r}$ from $u$ and $v$.\n",
    "\n",
    "Based on that, calculate $x_0$ and $x_1$ from $u = 0.5, v = 0.2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd5928",
   "metadata": {},
   "source": [
    "#### Formula for $\\hat{\\theta}$\n",
    "$\n",
    "\\begin{align*}\n",
    "    u &= p\\left(\\theta < \\hat{\\theta}\\right) \\\\\n",
    "    u &= \\int_0^{\\hat{\\theta}} \\frac{1}{2\\pi} d\\theta \\\\\n",
    "    u &= \\frac{\\hat{\\theta}}{2\\pi} \\\\\n",
    "    \\hat{\\theta} &= 2u\\pi\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "#### Formula for $\\hat{r}$\n",
    "The cdf for the exponential distribution is $\\text{cdf}(x) = 1 - e^{-\\lambda x}$\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "    v &= p\\left(\\frac{r^2}{2} < \\frac{\\hat{r}^2}{2} \\right) \\\\\n",
    "    v &= 1 - e^{-\\frac{\\hat{r}^2}{2}} \\\\\n",
    "    e^{-\\frac{\\hat{r}^2}{2}} &= 1 - v \\\\\n",
    "    -\\frac{\\hat{r}^2}{2} &= \\log(1-v) \\\\\n",
    "    \\hat{r} &= \\sqrt{-2 \\log(1-v)}\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ef55bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6680472308365775, 8.181219029232676e-17)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "u, v = 0.5, 0.2\n",
    "theta = 2 * u * math.pi\n",
    "r = math.sqrt(-2* math.log(1-v))\n",
    "x0 = r * math.cos(theta)\n",
    "x1 = r * math.sin(theta)\n",
    "x0,x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b95b5",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "Implement the sampling process for the standard normal distribution.\n",
    "\n",
    "Sample 10000 samples.and check the hypothesis that they are distributed according to a standard normal distribution using the [Kolmogorov- Smirnov Test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html) .\n",
    "\n",
    "We choose a confidence level of 95%; that is, we will reject the null hypothesis (our data is standard normal distributed) in favor of the alternative if the p-value is less than 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96be0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.008011850160992151, pvalue=0.5395197849329181)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import kstest, norm\n",
    "def sample_standard_normal(N:int=1) -> np.ndarray:\n",
    "    '''\n",
    "    Samples from a univariate standard normal distribution using Box-Muller transform.\n",
    "    \n",
    "    @Params:\n",
    "        N... number of samples\n",
    "        \n",
    "    @Returns:\n",
    "        Samples from a standard normal distribution\n",
    "    '''\n",
    "    unit_samples_u = np.random.random_sample(N)\n",
    "    unit_samples_v = np.random.random_sample(N)\n",
    "    normal_samples = []\n",
    "    for (u,v) in zip(unit_samples_u, unit_samples_v):\n",
    "        theta = 2 * u * math.pi\n",
    "        r = math.sqrt(-2* math.log(1-v))\n",
    "        sample = r * math.cos(theta)\n",
    "        normal_samples.append(sample)\n",
    "    return np.array(normal_samples)\n",
    "\n",
    "kstest(sample_standard_normal(10000), norm.cdf)\n",
    "# p-value > 0.05 => data is standard normal distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6166d78",
   "metadata": {},
   "source": [
    "## Arbitrary Gaussian\n",
    "Now we want to sample from an arbitrary, multivariate gaussian with mean vector $\\mu$ and covariance matrix $\\Sigma$.\n",
    "\n",
    "\n",
    "Let $X = (X_1,\\dots, X_n)$ be a vector of random variables $X_i$, whose joint density is a **standard** multivariate Gaussian. That is $X\\sim\\mathcal{N}(0, \\mathbb{1}_n)$.\n",
    "\n",
    "Then for a given mean vector $\\mu$ and a convariance matrix with [Cholesky decomposition](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cholesky.html) $\\Sigma = AA^T$ we can transform $X$ into \n",
    "\\begin{equation}\n",
    "Y = AX + \\mu\n",
    "\\end{equation}\n",
    "and $Y\\sim\\mathcal{N}(\\mu, \\Sigma)$\n",
    "### Task 4\n",
    "\n",
    "Implement the sampling routine to sample from a multivariate Gaussian with given mean and covariance matrix.\n",
    "\n",
    "Similar to Task 1, use this function to sample from a Gaussian with \n",
    "\\begin{align}\n",
    "\\mu &= [5, 2]^T\\\\\n",
    "\\Sigma &= \\begin{bmatrix}\n",
    "2.5&1.65\\\\\n",
    "1.65&1.93\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "and compare the true $\\mu$ and $\\Sigma$ to the maximum likelihood estimates based on an increasing number of samples over 10 tries.\n",
    "\n",
    "For visualization you can use the calculation of a mean confidence interval in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ec608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean: [5 2]\n",
      "True covariance: [[2.5  1.65]\n",
      " [1.65 1.93]]\n",
      "\n",
      "5 samples:\n",
      "Estimated mean: [4.64010487 1.911497  ]\n",
      "Estimated covariance: [[1.75288859 1.45266739]\n",
      " [1.45266739 2.02516743]]\n",
      "\n",
      "10 samples:\n",
      "Estimated mean: [4.56563275 2.27080314]\n",
      "Estimated covariance: [[0.713159   0.94898761]\n",
      " [0.94898761 1.47254133]]\n",
      "\n",
      "100 samples:\n",
      "Estimated mean: [4.96045534 2.13732045]\n",
      "Estimated covariance: [[2.80811182 2.01871447]\n",
      " [2.01871447 2.3630396 ]]\n",
      "\n",
      "500 samples:\n",
      "Estimated mean: [5.01752009 2.09367589]\n",
      "Estimated covariance: [[2.51780755 1.67742278]\n",
      " [1.67742278 1.99091663]]\n",
      "\n",
      "1000 samples:\n",
      "Estimated mean: [5.04082347 2.04953006]\n",
      "Estimated covariance: [[2.39421258 1.48347235]\n",
      " [1.48347235 1.76625033]]\n",
      "\n",
      "5000 samples:\n",
      "Estimated mean: [4.97621294 1.98308257]\n",
      "Estimated covariance: [[2.49590005 1.66967687]\n",
      " [1.66967687 1.94552181]]\n",
      "\n",
      "10000 samples:\n",
      "Estimated mean: [5.0137724  2.02189727]\n",
      "Estimated covariance: [[2.50908305 1.66376901]\n",
      " [1.66376901 1.92218696]]\n",
      "\n",
      "50000 samples:\n",
      "Estimated mean: [5.00138155 1.99748196]\n",
      "Estimated covariance: [[2.49721581 1.64661123]\n",
      " [1.64661123 1.91983525]]\n",
      "\n",
      "100000 samples:\n",
      "Estimated mean: [4.99719327 1.99629118]\n",
      "Estimated covariance: [[2.48434578 1.63782491]\n",
      " [1.63782491 1.91225237]]\n",
      "\n",
      "1000000 samples:\n",
      "Estimated mean: [4.99849272 1.99976431]\n",
      "Estimated covariance: [[2.49526606 1.64792532]\n",
      " [1.64792532 1.92711011]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sample_normal(mean:np.ndarray, cov:np.ndarray, N:int=1)-> np.ndarray:\n",
    "    '''\n",
    "    Samples from a multivariate normal distribution.\n",
    "    \n",
    "    @Params:\n",
    "        mean... mean vector of gaussian\n",
    "        cov... covariance matrix\n",
    "        N... number of samples\n",
    "        \n",
    "    @Returns:\n",
    "        samples from an arbitrary normal distribution\n",
    "    '''\n",
    "    num_dim = mean.shape[0]\n",
    "    standard_samples = sample_standard_normal(N)\n",
    "    for dim in range(1,num_dim):\n",
    "        standard_samples = np.vstack([standard_samples, sample_standard_normal(N)])\n",
    "    A = np.linalg.cholesky(cov)\n",
    "    return A @ standard_samples + np.atleast_2d(mean).T\n",
    "\n",
    "\n",
    "mean_true = np.array([5, 2])\n",
    "cov_true = np.array([[2.5 , 1.65], [1.65, 1.93]])\n",
    "num_samples = [5, 10, 100, 500, 1000, 5000, 10000, 50000, 100000, 1000000]\n",
    "print(f'True mean: {mean_true}')\n",
    "print(f'True covariance: {cov_true}')\n",
    "print()\n",
    "for n in num_samples:\n",
    "    print(f'{n} samples:')\n",
    "    samples = sample_normal(mean_true, cov_true, n)\n",
    "    mean_estimate = np.mean(samples, axis=1)\n",
    "    cov_estimate = np.cov(samples)\n",
    "    print(f'Estimated mean: {mean_estimate}')\n",
    "    print(f'Estimated covariance: {cov_estimate}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
